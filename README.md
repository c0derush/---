# 研发实习生实践题__日志分析

## 1.如何运行

python3.7版本及以上，无需额外第三方库。

```bash
chmod +x run.sh
./run.sh           # 默认读取 access.log
```

## 2.实现思路

逐行读取日志文件，分别计算四个指标：

```json
{
  "total_requests": 1234,
  "average_response_time_ms": 45.67,
  "status_code_counts": {"200": 1000, "404": 200, "500": 34},
  "busiest_hour": 14
}
```

我在代码中用于统计状态码和小时次数的核心数据结构是 **Python 的 `collections.Counter`**（底层是字典/哈希表）。这样可以实现高效计数，底层是字典（哈希表），查找/插入/更新都是平均 O(1) 时间复杂度，适合大日志文件逐行累加。



## 3.可以优化的地方

一个改进的方向是让脚本支持多种行格式，不仅限于每行纯 JSON。

做法大概包括：

1. 优先尝试 `json.loads(line)`。
2. 若失败，尝试从行中提取第一个 `{...}` 子串再 `json.loads`。
3. 支持 `--json-field NAME`：先解析整行为 JSON，再解析 `obj[NAME]` 中的 JSON 字符串。

当日志变得非常非常大（比如 10GB、100GB 或更高），当前实现会遇到以下问题：

- **I/O 和 CPU 瓶颈**：单线程 JSON 解析和磁盘读取可能变慢，处理时间很长。
- **容错与恢复**：处理过程中若失败（进程崩溃、节点断电），无断点恢复或部分结果合并机制导致重复工作或数据丢失。

解决的一种方法：把大文件按字节切成 N 个块（确保每块以完整行结束），为每个块启动一个独立进程并行解析该块内的 JSON 行并做局部聚合（例如：total、sum_rt、rt_count、status_counter、hour_counter）。所有子进程完成后在主进程合并这些局部聚合，得到全局结果。

大流量场景下也可以考虑使用Kafka，把日志写入 Kafka 主题，生产者负责可靠投递与分区，消费端用多个消费者（消费组）并行读取分区数据。





